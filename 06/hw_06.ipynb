{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: ML - Grundlagen und Algorithmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.) Reminders\n",
    "Please adhere to the hand-in conventions specified in the 0-th exercise sheet, i.p.,\n",
    "- You have to **submit the jupyter notebook file as well as the PDF**! \n",
    "- Please **adhere to the zip naming conventions**!\n",
    "\n",
    "The Exercise 1 Bayesian learning has been removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Gaussian Processes (5 Points)\n",
    "In this exercise, we will study the kernelized version of BLR, the Gaussian Process (GP). We will apply GPs on the same data set as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1) Gaussian Kernel (2 Points)\n",
    "Recall that for BLR we worked with a fixed set of features. In contrast, for GPs we define a discrepancy measure between input points, the so-called kernel function $k$, which by means of the kernel trick implicitly defines a (possibly infinite-dimensional) set of features. As we never have to explicitly evaluate these implicit features, we can work in very expressive feature spaces and still obtain a tractable algorithm. \n",
    "\n",
    "In the lecture, we defined the Gaussian kernel function as\n",
    "$$\n",
    "k(\\boldsymbol{x}, \\boldsymbol{x}') := \\lambda^{-1} \\exp\\left(-\\dfrac{||\\boldsymbol{x}-\\boldsymbol{x}'||^2}{2\\sigma^2}\\right), \\quad \\boldsymbol{x}, \\boldsymbol{x'} \\in \\mathbb R^d,\n",
    "$$\n",
    "where $\\lambda$ denotes the prior precision parameter and $\\sigma^2$ is the kernel bandwidth.\n",
    "\n",
    "Furthmerore, given training inputs $\\boldsymbol X = \\left\\{\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_N\\right\\}$, $\\boldsymbol{x}_i \\in \\mathbb R^d$, we defined the kernel vector as \n",
    "$$\n",
    "\\boldsymbol{k}(\\boldsymbol{x}) := \\left(k(\\boldsymbol{x}_1, \\boldsymbol{x}), \\dots, k(\\boldsymbol{x}_N, \\boldsymbol{x}) \\right)^T.\n",
    "$$\n",
    "Note that the dimension of the kernel vector is now determined by the number $N$ of training examples. In contrast, for BLR, the dimension of the feature vector $k$ was a fixed constant. We already discussed this distinction (parametric vs. non-parametric/instance-based methods) a number of times in the lecture.\n",
    "\n",
    "Finally, we define the *kernel matrix* as\n",
    "$$\n",
    "\\boldsymbol K := \n",
    "\\begin{pmatrix}\n",
    "  \\boldsymbol{k}(\\boldsymbol{x}_1) &  \\dots & \\boldsymbol{k}(\\boldsymbol{x}_N)\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "  k(\\boldsymbol{x}_1, \\boldsymbol{x}_1) & \\dots & k(\\boldsymbol{x}_1, \\boldsymbol{x}_N) \\\\\n",
    "  \\vdots & \\vdots & \\vdots \\\\\n",
    "  k(\\boldsymbol{x}_N, \\boldsymbol{x}_1) & \\dots & k(\\boldsymbol{x}_N, \\boldsymbol{x}_N) \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Implement the Gaussian kernel vector.\n",
    "\n",
    "**Hints**:\n",
    "- Note that you are supposed to compute the kernels for a batch of inputs $\\boldsymbol{x}$ and for a batch of inputs $\\boldsymbol{x'}$ in one function call.\n",
    "- As always, no for-loops are allowed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gaussian_kernel(x: np.ndarray, x_prime: np.ndarray, lam: float, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param x: first input (shape: [N_1, d])\n",
    "    :param x_prime: second input (shape: [N_2 x d])\n",
    "    :param lam: prior precision parameter (scalar)\n",
    "    :param sigma: bandwidth of the kernel (scalar)\n",
    "    :return: the Gaussian kernel, evaluated at all pairs (x, x') (shape: [N_1 x N_2])\n",
    "    \"\"\"\n",
    "    if len(x.shape) == 1:\n",
    "        x = x.reshape((-1, 1))\n",
    "\n",
    "    if len(x_prime.shape) == 1:\n",
    "        x_prime = x_prime.reshape((-1, 1))\n",
    "\n",
    "    ############################################################\n",
    "    # TODO Implement the Gaussian kernel\n",
    "    sq_dist = np.sum((x[:, np.newaxis, :] - x_prime[np.newaxis, :, :])**2, axis=2)\n",
    "    kernel = (1 / lam) * np.exp(-sq_dist / (2 * sigma ** 2))\n",
    "\n",
    "    #while i < x.shape[0]:\n",
    "    #   while j < x_prime.shape[0]:\n",
    "    #       exponent = np.exp(np.divide(-np.absolute(x[i] - x_prime[j]) ** 2, 2 * sigma ** 2))\n",
    "    #       kernel[i][j] = float(np.multiply(1 / lam, exponent))\n",
    "    #       j += 1\n",
    "    #   j = 0\n",
    "    #   i += 1\n",
    "\n",
    "    ############################################################\n",
    "\n",
    "    assert kernel.shape == (x.shape[0], x_prime.shape[0])\n",
    "    return kernel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `gaussian_kernel`, we can now easily compute the Gaussian kernel matrix. You do not need to implement anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_matrix(X: np.ndarray, lam: float, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param X: training data (shape: [N, d])\n",
    "    :param lam: prior precision parameter (scalar)\n",
    "    :param sigma: bandwidth of the kernel (scalar)\n",
    "    :return: the kernel matrix (N_train x N_train)\n",
    "    \"\"\"\n",
    "    return gaussian_kernel(x=X, x_prime=X, lam=lam, sigma=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2) Predictive Distribution for GPs (2 Points)\n",
    "From the lecture, we know the predictive mean and variance at a query input $\\boldsymbol{x}^* \\in \\mathbb R^d$ for GPs read:\n",
    "$$\n",
    "\\mu(\\boldsymbol{x}^*) = \\boldsymbol{k}(\\boldsymbol{x}^*)^T \\left(\\boldsymbol K + \\sigma_y^2 \\boldsymbol I \\right)^{-1} \\boldsymbol y, \\quad \\sigma(\\boldsymbol{x}^*) = \\sigma_y^2 + k(\\boldsymbol{x}^*, \\boldsymbol{x}^*) - \\boldsymbol{k}(\\boldsymbol{x}^*)^T \\left(\\boldsymbol K + \\sigma_y^2 \\boldsymbol I \\right)^{-1} \\boldsymbol{k}(\\boldsymbol{x}^*),\n",
    "$$\n",
    "where $\\boldsymbol{y} := \\left(y_1, \\dots, y_N \\right)^T$ are the targets corresponding to $\\boldsymbol X$ and $\\sigma_y^2$ is the variance of the likelihood.\n",
    "\n",
    "Implement the predictive distribution for GPs.\n",
    "\n",
    "**Hints:** \n",
    "- We defined the likelihood variance $\\sigma_y^2$ as a global variable at the beginning of this notebook and fixed it to the true noise variance of 1.0, which we assume to be known for this exercise. \n",
    "- Avoid duplicate or unneccessary computations, and use numerically stable operations instead of computing inverses, if possible!\n",
    "- Make use of both `gaussian_kernel` and `gaussian_kernel_matrix`!\n",
    "- Do not use any for loops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_predictive_distribution(x: np.ndarray, y: np.ndarray, X: np.ndarray, lam: float, sigma: float):\n",
    "    \"\"\"\"\n",
    "    :param x: query inputs (shape: [N_q, d])\n",
    "    :param X: training inputs (shape: [N, d])\n",
    "    :param y: training targets (shape: [N, 1])\n",
    "    :param lam: prior precision parameter (scalar)\n",
    "    :param sigma: bandwidth of the kernel (scalar)\n",
    "    :return: the mean (shape: [N_q])\n",
    "             the variance (shape: [N_q])\n",
    "             of the predictive distribution\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO Implement the predictive distribution for GPs\n",
    "    ############################################################\n",
    "\n",
    "    assert mean_x.shape == (x.shape[0],)\n",
    "    assert var_x.shape == (x.shape[0],)\n",
    "    return mean_x, var_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again plot the predictive distribution for different values of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lambdas and sigmas\n",
    "lambdas = [1e-5, 1e-3, 1.0, 10.0]\n",
    "feature_sigmas = [0.1, 0.5, 1.0, 10.0]\n",
    "\n",
    "# compute predictive distribution and function samples for the lambdas and sigmas\n",
    "pred_means, pred_vars, labels = [], [], []\n",
    "for lam, feature_sigma in product(lambdas, feature_sigmas):\n",
    "    # obtain the predictive distribution\n",
    "    pred_mean, pred_var = gp_predictive_distribution(x=x_plot, X=x_train, y=y_train, lam=lam, sigma=feature_sigma)\n",
    "\n",
    "    # collect computed stuff\n",
    "    pred_means.append(pred_mean)\n",
    "    pred_vars.append(pred_var)\n",
    "    labels.append(f\"$\\lambda$ = {lam}, $\\sigma$ = {feature_sigma}\")\n",
    "\n",
    "# plot \n",
    "fig, axes = plt.subplots(nrows=len(lambdas), ncols=len(feature_sigmas), sharex=True,\n",
    "                         figsize=(len(feature_sigmas) * 5, len(lambdas) * 3), squeeze=False)\n",
    "for i, (pred_mean, pred_var, funcs, label) in enumerate(zip(pred_means, pred_vars, funcss, labels)):\n",
    "    ax = axes[i // len(feature_sigmas), i % len(feature_sigmas)]\n",
    "\n",
    "    # the predictive distribution together with the 95% confidence interval\n",
    "    ax.plot(x_plot, pred_mean, 'b', label=label)\n",
    "    ax.fill_between(np.squeeze(x_plot), np.squeeze(pred_mean) - 2 * np.sqrt(pred_var),\n",
    "                    np.squeeze(pred_mean) + 2 * np.sqrt(pred_var), alpha=0.2, color='blue')\n",
    "    ax.plot(x_train, y_train, 'or')\n",
    "    ax.plot(x_plot, y_plot, 'black')\n",
    "\n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3) Sampling Functions from a GP (1 Point)\n",
    "For BLR, we could obtain function samples, evaluated at query inputs, by sampling weights from the posterior and transforming them to function evaluations at the query inputs (cf. Exercise 1.4). Explain how this works for GPs! I.e., how we can obtain function samples, evaluated at query inputs, from GPs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Bayesian Neural Networks (BNN) 10 Points\n",
    "\n",
    "\n",
    "## Guideline\n",
    "In this exercise, you are going to implement and test a few BNN algorithms and compare their predictions in a binary classification task. We use PyTorch as our neural network toolbox and structure the algorithms using the Object-Oriented-Programming manner.\n",
    "\n",
    "The algorithms include: MAP, SWAG, MCD and their corresponding ensembles, i.e. Deep Ensemble, Multi-SWAG, Multi-MCD. \n",
    "<br>\n",
    "\n",
    "### Big Picture and Workflow\n",
    "- We offered a few utilities functions to load dataset, evaluate models and plot classification results.\n",
    "- We offered the Maximum A Posteriori (MAP) binary classifier as our base algorithm.\n",
    "- Consider the time constraint of the last homework, we decided to offer the implementation of SWAG and Multi-SWAG. \n",
    "- You are going to implement a class **EnsembleWrapper** which can store multiple instances of a single algorithm to make them an ensemble model.\n",
    "- You are going to implement the MCD methods. It, together with the SWAG, inherit the MAP class and can thus reuse part of the code.\n",
    "<br>\n",
    "\n",
    "### Acknowledgement\n",
    "Special thanks to Florian Seligmann. Some of the content in this homework is adapted from his bachelor's thesis codebase.\n",
    "\n",
    "### Install Dependencies\n",
    "We need to install a package **tqdm** to show the progress bar of our training and plotting process.\n",
    "Call **conda install -c conda-forge tqdm** to install it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Plot Dataset\n",
    "We will use our best friend **Two-Moon Dataset** for our homework. This time, we removed some data points in the train dataset (marked in red circle), so the data in the validation set contains some OOD data points (out of distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from util import plot_dataset\n",
    "\n",
    "two_moon_dataset = np.load(\"bnn_dataset.npz\", allow_pickle=True)\n",
    "train_data_x, train_data_y = two_moon_dataset[\"train_data_x\"], two_moon_dataset[\n",
    "    \"train_data_y\"]\n",
    "val_data_x, val_data_y = two_moon_dataset[\"val_data_x\"], two_moon_dataset[\n",
    "    \"val_data_y\"]\n",
    "test_data_x, test_data_y = two_moon_dataset[\"test_data_x\"], two_moon_dataset[\n",
    "    \"test_data_y\"]\n",
    "\n",
    "# Transfer numpy array into torch tensor\n",
    "train_samples_tensor = torch.as_tensor(train_data_x, dtype=torch.float32)\n",
    "train_labels_tensor = torch.as_tensor(train_data_y, dtype=torch.long)\n",
    "test_samples_tensor = torch.as_tensor(val_data_x, dtype=torch.float32)\n",
    "test_labels_tensor = torch.as_tensor(val_data_y, dtype=torch.long)\n",
    "\n",
    "# Plotting\n",
    "plot_dataset(train_data_x, train_data_y, val_data_x, val_data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "We define a few hyperparameters here. You can change them to see how they affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05  # Learning rate\n",
    "wd = 1e-5  # Weight decay, the L2 regularization term\n",
    "num_h_neuron = 16  # Number of neurons in the hidden layers of the NN\n",
    "dropout_p = 0.25  # The probability of a neuron being dropped out\n",
    "max_epoch = 800  # Max training episode\n",
    "seed = 0  # Random seed to ensure reproducibility\n",
    "swag_start = int(0.4 * max_epoch)  # From this epoch will SWAG start updating\n",
    "swag_update_interval = 5  # SWAG update interval\n",
    "n_ensemble = 5  # Number of method instances in the Multi-X models.\n",
    "num_smp = 100  # Number of parameter samples from the posterior distribution of each single method instance\n",
    "plot_density = 0.02  # The pixel size of the contour. Use bigger values if you face problems in plotting the result.\n",
    "num_minibatch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum A Posteriori\n",
    "We define **MAPBinaryClassifier** to perform binary classification following the MAP manner. As introduced in the lecture, MAP is equivalent to train a neural network with L2 regularization.\n",
    "This class includes 4 functions:\n",
    "- init: instantiate one net and one optimizer for the model\n",
    "- train_one_epoch: apply one train iteration using the training and validation datasets. Here, as split our train dataset into small mini-batch while feed the entire validation set without using minibatch.\n",
    "- step: given input and label, compute the loss function and accuracy of the classification\n",
    "- inference: given input, compute the probabilities of classification\n",
    "\n",
    "This class serves as the super class and will be further extended in **SWAGBinaryClassifier** and **MCDBinaryClassifier** classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "class MAPBinaryClassifier:\n",
    "    def __init__(self, **kwargs):\n",
    "        # nn.Sequential is a container for a sequence of neural network layers\n",
    "        # Call self.net(x) will sequentially apply the layers to the input x\n",
    "        self.net = nn.Sequential(nn.Linear(2, num_h_neuron),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_h_neuron, num_h_neuron),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_h_neuron, num_h_neuron),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_h_neuron, 2),\n",
    "                                 nn.LogSoftmax(dim=-1))\n",
    "\n",
    "        # SGD Optimizer, lr is the learning rate,\n",
    "        # weight_decay is the L2 regularization factor\n",
    "        self.optimizer = SGD(self.net.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    def train_one_epoch(self, train_x, train_y, val_x, val_y):\n",
    "        # Training\n",
    "        self.net.train()\n",
    "\n",
    "        num_data = train_x.shape[0]\n",
    "        perm_idx = torch.randperm(num_data)\n",
    "        mini_batch_size = num_data // num_minibatch\n",
    "        b_idx = mini_batch_size * num_minibatch\n",
    "        x_batches = torch.chunk(train_x[perm_idx][:b_idx], num_minibatch)\n",
    "        y_batches = torch.chunk(train_y[perm_idx][:b_idx], num_minibatch)\n",
    "        train_accuracies = torch.zeros(num_minibatch)\n",
    "        for i, (x, y) in enumerate(zip(x_batches, y_batches)):\n",
    "            loss, accuracy = self.step(x, y)\n",
    "            train_accuracies[i] = accuracy\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        train_accuracy = train_accuracies.mean().item()\n",
    "\n",
    "        # Validation\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            _, val_accuracy = self.step(val_x, val_y)\n",
    "\n",
    "        # Return\n",
    "        return train_accuracy, val_accuracy\n",
    "\n",
    "    def step(self, x, y):\n",
    "        \"\"\"\n",
    "        Given input and label, compute the cross-entropy loss function and accuracy\n",
    "        \"\"\"\n",
    "        pred_log_prob = self.net(x)\n",
    "        pred_class = pred_log_prob.argmax(dim=-1)\n",
    "        loss = nn.functional.nll_loss(pred_log_prob, y)\n",
    "        num_data = x.shape[0]\n",
    "        num_pred_correct = (pred_class == y).sum().item()\n",
    "        accuracy = num_pred_correct / num_data\n",
    "        return loss, accuracy\n",
    "\n",
    "    # The method decorator @torch.no_grad() disables gradient computation\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x):\n",
    "        self.net.eval()  # Set the model to evaluation mode\n",
    "        # Compute the probability of each class given the log-probability\n",
    "        pred_prob = self.net(x).exp()\n",
    "        return pred_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Ensemble\n",
    "A Deep Ensemble model contains multiple MAP model instances. When doing inference, the predictions of each instance will be averaged as the resulting prediction.\n",
    "\n",
    "### Task 3.1: Ensemble wrapper (4 Points)\n",
    "Indeed, any Multi-X methods can be implemented in a similar way, i.e. using an ensemble wrapper to make a single method \"Multi\". Therefore, in this homework, we will firstly implement this Ensemble Wrapper, and then always use this wrapper to instantiate our method. If the number of ensemble instances is 1, the resulting method is just a single method. Otherwise, it is a Multi-X method.\n",
    "\n",
    "For convenience and simplicity, we also implement the main training loop in this wrapper class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm is a package offering nice look progress bar\n",
    "from tqdm import tqdm\n",
    "# We always set the random seed to ensure reproducibility\n",
    "from util import set_random_seed_globally\n",
    "\n",
    "\n",
    "class EnsembleWrapper:\n",
    "    def __init__(self, num_ensemble, model_class, **kwargs):\n",
    "        \"\"\"\n",
    "        Create method ensemble\n",
    "        Args:\n",
    "            num_ensemble: the number of base models, 1 for single method, > 1 for Multi-method\n",
    "            model_class: class name of the single method\n",
    "            **kwargs: other keyword arguments specific to the single method init function\n",
    "        \"\"\"\n",
    "        self.n_ensemble = num_ensemble\n",
    "        set_random_seed_globally(seed)\n",
    "        self.models = [model_class(**kwargs) for _ in range(num_ensemble)]\n",
    "\n",
    "    def train_model(self, train_x, train_y, val_x, val_y):\n",
    "        \"\"\"\n",
    "        Train all model instances in the ensemble\n",
    "        \"\"\"\n",
    "\n",
    "        # Loop over models\n",
    "        for i, model in enumerate(self.models):\n",
    "\n",
    "            # Loop over training iterations\n",
    "            pbar = tqdm(range(max_epoch))  # progress bar\n",
    "\n",
    "            for epoch in pbar:\n",
    "                train_accuracy, val_accuracy = \\\n",
    "                    model.train_one_epoch(train_x, train_y, val_x, val_y)\n",
    "\n",
    "                # Logging and updating progress bar\n",
    "                pbar.set_postfix({\"Model\": i + 1, \"Epoch\": epoch + 1,\n",
    "                                  \"train_accuracy\": train_accuracy,\n",
    "                                  \"val_accuracy\": val_accuracy})\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ensemble_inference(self, x):\n",
    "        \"\"\"\n",
    "        Aggregate inference result of each model instance\n",
    "        \"\"\"\n",
    "        ######################### Your code starts here #########################\n",
    "        # Loop over each method instance\n",
    "\n",
    "        # Aggregate the predictions\n",
    "\n",
    "        ######################### Your code ends here #########################\n",
    "        return ensemble_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP and DeepEnsemble-5 instances\n",
    "Verify your implementation by training a MAP instance and a DeepEnsemble-5 instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# MAP, num_ensemble = 1\n",
    "max_a_p = EnsembleWrapper(num_ensemble=1,\n",
    "                          model_class=MAPBinaryClassifier)\n",
    "print(\"Training MAP\")\n",
    "time.sleep(0.3)  # Sleep for 0.3 second to ensure the print function won't break the progress bar\n",
    "max_a_p.train_model(train_samples_tensor, train_labels_tensor,\n",
    "                    test_samples_tensor, test_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Ensemble, we ensemble 5 MAP instances\n",
    "deep_ensemble_5 = EnsembleWrapper(num_ensemble=n_ensemble,\n",
    "                                  model_class=MAPBinaryClassifier)\n",
    "print(\"Training DeepEnsemble5\")\n",
    "time.sleep(0.3)\n",
    "deep_ensemble_5.train_model(train_samples_tensor, train_labels_tensor,\n",
    "                            test_samples_tensor, test_labels_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Weight Averaging Gaussian (SWAG)\n",
    "SWAG uses the parameters of the NN at different training stages to fit the approximated posterior distribution, known as a Multivariate Gaussian distribution. We implement it by using and extending the MAP as the super class.\n",
    "<br>\n",
    "\n",
    "The training procedure of a SWAG can be described as:\n",
    "- Train a SWAG as we train an MAP classifier until convergence.\n",
    "- Then keep training it and collect the parameters of the NN at different further $T$ training stages.\n",
    "- The parameters collected at a certain stage (reshaped as a parameter vector $\\boldsymbol\\theta_i$ ) can be considered as a sample of the approximated posterior distribution.\n",
    "- We use all these collected samples to compute the posterior distribution, in terms of the mean and covariance.\n",
    "- With mean $\\overline{\\boldsymbol\\theta} = \\frac{1}{T} \\sum^T_{i=1}\\boldsymbol\\theta_i$ and covariance $\\boldsymbol\\Sigma = \\frac{1}{T-1} \\sum^T_{i=1}(\\boldsymbol\\theta_i - \\boldsymbol\\theta)(\\boldsymbol\\theta_i - \\boldsymbol\\theta)^\\intercal = \\frac{1}{T-1} \\boldsymbol{D} \\boldsymbol{D}^\\intercal$\n",
    "- To decrease memory usage, we avoid saving all individual samples. Instead, each time when we get a new sample, we update a few intermediate variables, including the running mean $\\overline{\\boldsymbol\\theta}$, mean of square $\\overline{\\boldsymbol\\theta^2}$ and the deviation $\\boldsymbol d$ of the parameter vector. We store the last K deviations, resulting a low-rank deviation matrix $\\boldsymbol{D}_{lr}$.\n",
    "\n",
    "\\begin{align*}\n",
    "        \\text{Update mean:} \\quad \\overline{\\boldsymbol\\theta} \\leftarrow \\frac{i \\overline {\\boldsymbol\\theta} + {\\boldsymbol\\theta_i}}{i + 1} \\\\\n",
    "        \\text{Update mean square:} \\quad \\overline{\\boldsymbol\\theta^2} \\leftarrow \\frac{i \\overline {\\boldsymbol\\theta^2} + {\\boldsymbol\\theta^2_i}}{i + 1} \\\\\n",
    "        \\text{Compute deviation:} \\quad {\\boldsymbol d_i} = \\boldsymbol\\theta - \\overline{\\boldsymbol\\theta_i}\\\\\n",
    "        \\text{Store deviation:} \\quad {\\boldsymbol D_{lr}}.\\text{pop(0)}, \\quad {\\boldsymbol D_{lr}}.\\text{add}{(\\boldsymbol d_i)}\n",
    "\\end{align*}\n",
    "<br>\n",
    "\n",
    "The inference procedure can be described as:\n",
    "- Approximate the covariance matrix $\\boldsymbol\\Sigma$ using the low-rank deviation matrix $\\boldsymbol{D}_{lr}$ and a diagonal covariance matrix $\\boldsymbol\\Sigma_{\\text{diag}} = \\text{diag}(\\overline{\\boldsymbol\\theta^2} - {\\overline{\\boldsymbol\\theta}}^2)$.\n",
    "- Form up the posterior distribution using $\\overline{\\boldsymbol\\theta}$ and approximated $\\boldsymbol\\Sigma$.\n",
    "- Generate parameter samples using the posterior, iteratively reset the network and make a prediction using these parameter samples.\n",
    "- Averaging all these individual predictions and use the result as the final prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Read the SWAG implementation (0 Point)\n",
    "We decide to offer the implementation to you, due to the time constraint of the last homework..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two utility functions to manipulate the parameters of the NN\n",
    "from util import parameters_to_vector, set_params_to_net\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object-Oriented-Programming (OOP)\n",
    "# We inherit MAPBinaryClassifier as our super class of SWAG\n",
    "class SWAGBinaryClassifier(MAPBinaryClassifier):\n",
    "    def __init__(self, num_samples, swag_start_epoch, update_interval, k=30):\n",
    "        # We reuse the init function of the super class to create our NN and optimizer (OOP)\n",
    "        super().__init__()\n",
    "\n",
    "        # The last K columns of the Deviation Matrix\n",
    "        self.K = k\n",
    "\n",
    "        # The intermediate running variables, later will be used to compute the posterior distribution\n",
    "        # The mean of the parameter samples\n",
    "        # Shape: [num_params]\n",
    "        self.swag_mean = parameters_to_vector(self.params)\n",
    "\n",
    "        # The mean of the squared parameter samples\n",
    "        # Shape: [num_params]\n",
    "        self.swag_square = self.swag_mean.pow(2)\n",
    "\n",
    "        # The Low rank deviation matrix, storing the K latest deviations (parameter sample to the mean)\n",
    "        # Shape: [num_params, K]\n",
    "        self.swag_dev = torch.zeros((self.swag_mean.shape[0], self.K))\n",
    "\n",
    "        # Helper variables to determine when and how frequent to update the SWAG distribution variables\n",
    "        self.swag_start_epoch = swag_start_epoch\n",
    "        self.update_interval = update_interval\n",
    "        self.num_swag_update = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        # In the inference, how many parameter samples are going to be sampled from the computed posterior distribution\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        Return the parameters managed by the optimizer, which are the same parameters stored in the NN.\n",
    "        \"\"\"\n",
    "        return self.optimizer.param_groups\n",
    "\n",
    "    def train_one_epoch(self, train_x, train_y, val_x, val_y):\n",
    "        \"\"\"\n",
    "        Train one epoch similar to MAP.\n",
    "        If the update condition of SWAG has been satisfied, then update the SWAG variables.\n",
    "\n",
    "        This function will override the namesake function defined in the super class (OOP).\n",
    "        \"\"\"\n",
    "\n",
    "        # Update network as MAP, reuse the super class function\n",
    "        train_accuracy, val_accuracy = super().train_one_epoch(train_x, train_y,\n",
    "                                                               val_x, val_y)\n",
    "\n",
    "        # Update SWAG if certain conditions have been satisfied\n",
    "        if self.current_epoch >= self.swag_start_epoch \\\n",
    "                and self.current_epoch % self.update_interval == 0:\n",
    "            self.update_swag_params()\n",
    "        self.current_epoch += 1\n",
    "\n",
    "        # Return\n",
    "        return train_accuracy, val_accuracy\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_swag_params(self):\n",
    "        \"\"\"\n",
    "        Update SWAG intermediate variables using the latest parameters stored in the NN\n",
    "        \"\"\"\n",
    "        # We flatten all parameters from different NN layers into a parameter vector\n",
    "        params_vec = parameters_to_vector(self.params)\n",
    "\n",
    "        ######################### Our code starts here #########################\n",
    "        # Update the mean of the parameter samples\n",
    "        self.swag_mean = (self.num_swag_update * self.swag_mean + params_vec) \\\n",
    "                         / (self.num_swag_update + 1)\n",
    "\n",
    "        # Update the mean of the squared parameter samples\n",
    "        self.swag_square = (self.num_swag_update * self.swag_square \\\n",
    "                            + params_vec.pow(2)) / (self.num_swag_update + 1)\n",
    "\n",
    "        # Update the Deviation matrix by only keeping the latest K columns and discard the rest\n",
    "        # You may need to use torch.roll() to shift the columns and add the new deviation vector to the last column\n",
    "        self.swag_dev = torch.roll(self.swag_dev, -1, 1)\n",
    "        self.swag_dev[:, -1] = params_vec - self.swag_mean\n",
    "        ######################### Our code ends here #########################\n",
    "\n",
    "        # Increase the number of SWAG updates\n",
    "        self.num_swag_update += 1\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_approx_posterior(self):\n",
    "        \"\"\"\n",
    "        Fit the posterior distribution, modeled as a low-rank multivariate Gaussian distribution\n",
    "        \"\"\"\n",
    "        eps = 1e-6  # For numerical stability\n",
    "        cov_diag = 0.5 * torch.relu((self.swag_square - self.swag_mean.pow(2)) + eps)\n",
    "        cov_factor = self.swag_dev / math.sqrt(2 * (self.K - 1))\n",
    "        posterior = torch.distributions.LowRankMultivariateNormal(\n",
    "            self.swag_mean, cov_factor, cov_diag, validate_args=False)\n",
    "        return posterior\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x):\n",
    "        \"\"\"\n",
    "        Make prediction given input.\n",
    "        This function will override the namesake function in the super class (OOP).\n",
    "        \"\"\"\n",
    "\n",
    "        ######################### Our code starts here #########################\n",
    "        # Get the posterior distribution\n",
    "        posterior = self.get_approx_posterior()\n",
    "\n",
    "        # Sample parameter samples\n",
    "        params_smp = posterior.sample([self.num_samples])\n",
    "\n",
    "        # Make a tensor to store all predictions\n",
    "        predictions = torch.empty(self.num_samples, x.shape[0], 2)\n",
    "\n",
    "        # Compute and store each individual prediction\n",
    "        for i, smp in enumerate(params_smp):\n",
    "            # Set parameter sample to NN\n",
    "            set_params_to_net(smp, self.params)\n",
    "            self.net.eval()  # Optional\n",
    "\n",
    "            # Make and store prediction using the network\n",
    "            predictions[i] = super().inference(x)\n",
    "\n",
    "        ######################### Our code ends here #########################\n",
    "\n",
    "        return predictions.mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWAG and Multi-SWAG instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single SWAG\n",
    "swag = EnsembleWrapper(num_ensemble=1,\n",
    "                       model_class=SWAGBinaryClassifier,\n",
    "                       num_samples=num_smp,\n",
    "                       swag_start_epoch=swag_start,\n",
    "                       update_interval=swag_update_interval)\n",
    "print(\"Training SWAG\")\n",
    "time.sleep(0.3)\n",
    "swag.train_model(train_samples_tensor, train_labels_tensor,\n",
    "                 test_samples_tensor, test_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi SWAG, num_ensemble = 5 (by default)\n",
    "multi_swag_5 = EnsembleWrapper(num_ensemble=n_ensemble,\n",
    "                               model_class=SWAGBinaryClassifier,\n",
    "                               num_samples=num_smp,\n",
    "                               swag_start_epoch=swag_start,\n",
    "                               update_interval=swag_update_interval)\n",
    "print(\"Training Multi-SWAG-5\")\n",
    "time.sleep(0.3)\n",
    "multi_swag_5.train_model(train_samples_tensor, train_labels_tensor,\n",
    "                         test_samples_tensor, test_labels_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Dropout (MCD)\n",
    "Dropout is a regularization technique used in NN to mitigate overfitting. It randomly deactivates a fraction of the units during training, forcing the network to learn more robust representations. This prevents co-adaptation of neurons and improves generalization. In inference, the dropout is often disabled. Monte Carlo dropout extends this approach by performing dropout even in inference, and average the result of multiple forward passes to provide a measure of uncertainty.\n",
    "\n",
    "In PyTorch, eval() and train() are methods used to control the behavior of a neural network model.\n",
    "eval() sets the NN in evaluation mode and disables certain operations like dropout and batch normalization, ensuring deterministic behavior and consistent results during inference. train() sets the NN in training mode and enables dropout and batch normalization to improve generalization. Based on the above information, how can we enable dropout in inference mode?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Monte-Carlo Dropout (MCD) implementation (6 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object-Oriented-Programming (OOP)\n",
    "# We inherit MAPBinaryClassifier as our super class of MCD\n",
    "class MCDBinaryClassifier(MAPBinaryClassifier):\n",
    "    def __init__(self, num_samples):\n",
    "        # We override the init function of the super class to create our NN with Dropout layers\n",
    "        self.net = nn.Sequential(nn.Linear(2, num_h_neuron),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout_p),\n",
    "                                 nn.Linear(num_h_neuron, num_h_neuron),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout_p),\n",
    "                                 nn.Linear(num_h_neuron, num_h_neuron),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout_p),\n",
    "                                 nn.Linear(num_h_neuron, 2),\n",
    "                                 nn.LogSoftmax(dim=-1))\n",
    "\n",
    "        self.optimizer = SGD(self.net.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        # Number of samples used in inference\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x):\n",
    "        \"\"\"\n",
    "        Make prediction given input.\n",
    "        This function will override the namesake function in the super class (OOP).\n",
    "        \"\"\"\n",
    "        ######################### Your code starts here #########################\n",
    "        # Set model train mode to enable dropout in inference\n",
    "\n",
    "        # Make a tensor to store all predictions\n",
    "\n",
    "        # Compute and store each individual prediction\n",
    "        ######################### Your code ends here #########################\n",
    "        return predictions.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCD and Multi-MCD instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcd = EnsembleWrapper(num_ensemble=1,\n",
    "                      model_class=MCDBinaryClassifier,\n",
    "                      num_samples=num_smp)\n",
    "print(\"Training MCD\")\n",
    "time.sleep(0.3)\n",
    "mcd.train_model(train_samples_tensor, train_labels_tensor,\n",
    "                test_samples_tensor, test_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_mcd_5 = EnsembleWrapper(num_ensemble=n_ensemble,\n",
    "                              model_class=MCDBinaryClassifier,\n",
    "                              num_samples=num_smp)\n",
    "print(\"Training Multi-MCD-5\")\n",
    "time.sleep(0.3)\n",
    "multi_mcd_5.train_model(train_samples_tensor, train_labels_tensor,\n",
    "                        test_samples_tensor, test_labels_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and compare all models\n",
    "**This may take a few minutes to finish**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import plot_all_models, evaluate_models\n",
    "\n",
    "model_dict = {\n",
    "    \"MAP\": max_a_p,\n",
    "    \"DeepEnsemble5\": deep_ensemble_5,\n",
    "    \"SWAG\": swag,\n",
    "    \"Multi-SWAG-5\": multi_swag_5,\n",
    "    \"MCD\": mcd,\n",
    "    \"Multi-MCD-5\": multi_mcd_5\n",
    "}\n",
    "evaluate_models(model_dict, test_data_x, test_data_y)\n",
    "plot_all_models(model_dict, train_data_x, train_data_y, plot_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have done everything correctly, you should gain about 87-95% of the test accuracy for all models. We also reported the adapted callibration error (ACE), where the implementation can be found in the util file. We can see the Multi-X model in general achieves smaller errors than its single counterpart.\n",
    "\n",
    "The resulting classification boundaries of all models should be very different. The MAP has a very narrow boundary while the others get more uncertainty in the area that is far from the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
