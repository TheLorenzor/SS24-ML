{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e7e4d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exercise 5 - Maschinelles Lernen: Grundlagen und Algorithmen (18P)\n",
    "In this exercise you will implement two generative models: First a Gaussian mixture model (GMM, 7 Points) and second, a Denoising Diffusion Probabillistic Model (DDPM, 11 Points).\n",
    "\n",
    "As usual, we start by importing some packages and loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b75f6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664eaa44",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.load(\"samples_u.npy\")\n",
    "seed = 123\n",
    "k = 3\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1], label='Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b150fb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1.) Expectation Maximization for Gaussian Mixture Models (7 Points)\n",
    "In this exercise, we implement the expectation maximization (EM) algorithm to fit a Gaussian mixture model (GMM) to data. We start with an implementation of the log-density of a single Gaussian. We already saw an implementation of this in the first exercise and noted there that it was not the \"proper\" way of doing it. Here, we provide a better implementation. Compare the two implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7dc214",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_log_density(samples: np.ndarray, mean: np.ndarray, covariance: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the log-density of samples under a Gaussian distribution in a stable and efficient manner.\n",
    "    :param samples: samples (shape: [N x dim])\n",
    "    :param mean: mean of the distribution (shape: [dim])\n",
    "    :param covariance: covariance of the distribution (shape: [dim x dim])\n",
    "    :return: log N(x | mean, covariance) (shape: [N])\n",
    "    \"\"\"\n",
    "    dim = mean.shape[0]\n",
    "\n",
    "    # Recall:\n",
    "    #  log N(x | mu, Sigma) = - 0.5 * dim * log(2pi)\n",
    "    #                         - 0.5 * log det Sigma\n",
    "    #                         - 0.5 * (x - mu)^T Sigma^(-1) (x - mu)\n",
    "\n",
    "    # Everything is more stable using the Cholesky decomposition!\n",
    "    #  Sigma = L^T L, where L is lower triangular\n",
    "    chol_covariance = np.linalg.cholesky(covariance)\n",
    "\n",
    "    # compute constant term: -0.5 * dim * log(2pi)\n",
    "    const_term = dim * np.log(2 * np.pi)  # float\n",
    "\n",
    "    # compute log-determinant: -0.5 * log det Sigma = -0.5 * log prod diag(L)\n",
    "    logdet_term = 2 * np.sum(np.log(np.diagonal(chol_covariance) + 1e-25))  # (N,)\n",
    "\n",
    "    # compute exponential term:\n",
    "    #  (x - mu)^T Sigma^(-1) (x - mu) = ((x - mu) L^(-1))^T * (L^(-1) (x - mu))\n",
    "    diff = samples - mean[None, :]  # shape (N, dim)\n",
    "    sol = scipy.linalg.solve_triangular(chol_covariance, diff.T, lower=True).T  # (N, dim)\n",
    "    exp_term = np.sum(np.square(sol), axis=-1)  # (N,)\n",
    "    return -0.5 * (const_term + logdet_term + exp_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d980525d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We also provide some plotting functionaliy for 2D GMMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818bbb5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "COLORS = ['tab:blue', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:grey',\n",
    "          'tab:olive', 'tab:cyan']\n",
    "\n",
    "def visualize_2d_gmm(samples, weights, means, covs, title):\n",
    "    \"\"\"Visualize the model and the samples.\"\"\"\n",
    "    plt.figure(figsize=[7,7])\n",
    "    plt.title(title)\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], label=\"Samples\", c=COLORS[0])\n",
    "\n",
    "    for i in range(means.shape[0]):\n",
    "        (largest_eigval, smallest_eigval), eigvec = np.linalg.eig(covs[i])\n",
    "        phi = -np.arctan2(eigvec[0, 1], eigvec[0, 0])\n",
    "\n",
    "        plt.scatter(means[i, 0:1], means[i, 1:2], marker=\"x\", c=COLORS[i+1])\n",
    "\n",
    "        a = 2.0 * np.sqrt(largest_eigval)\n",
    "        b = 2.0 * np.sqrt(smallest_eigval)\n",
    "\n",
    "        ellipse_x_r = a * np.cos(np.linspace(0, 2 * np.pi, num=200))\n",
    "        ellipse_y_r = b * np.sin(np.linspace(0, 2 * np.pi, num=200))\n",
    "\n",
    "        R = np.array([[np.cos(phi), np.sin(phi)], [-np.sin(phi), np.cos(phi)]])\n",
    "        r_ellipse = np.array([ellipse_x_r, ellipse_y_r]).T @ R\n",
    "        plt.plot(means[i, 0] + r_ellipse[:, 0], means[i, 1] + r_ellipse[:, 1],\n",
    "                 label=\"Component {:02d}, Weight: {:0.4f}\".format(i, weights[i]), c=COLORS[i+1])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d57626",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "For this exercise, you need to implement 4 functions:\n",
    "- The log joint densities $\\log \\left( p(x | z = k) p(z = k) \\right)$ for all components $k$ of a GMM,\n",
    "- the log-likelihood $\\log p(x) = \\log \\sum_k p(x | z = k) p(z = k) $ of a GMM,\n",
    "- the E-Step of the EM algorithm for GMMs,\n",
    "- the M-Step of the EM algorithm for GMMs.\n",
    "\n",
    "***Hints***:\n",
    "- for-loops are only allowed in ```log_joint_densities()```!\n",
    "- Consider using the ```logsumexp```-trick for a stable implementation of ```gmm_log_likelihood()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c83195",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_log_joint_densities(samples: np.ndarray, weights: np.ndarray, means: np.ndarray, covariances: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the log joint densities of samples under a GMM, given model parameters.\n",
    "    :param samples: samples (shape: [N x dim])\n",
    "    :param weights: mixture weights, i.e., p(z) (shape: [num_components])\n",
    "    :param means: component means, i.e., means of p(x|z) (shape: [num_components, dim])\n",
    "    :param covariances: component covariances, i.e., means of p(x|z) (shape: [num_components, dim, dim])\n",
    "    :return: log joint densities, i.e., log (p(x|z = k)p(z = k)) (shape: [N, k])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO: Compute the log joint densities log ( p(x_n | z = k) p(z = k) ) of a GMM.\n",
    "    \n",
    "    # compute log component densities: log p(x_n | z = k) = log N(x_n | mu_k, cov_k)\n",
    "    \n",
    "    # compute log joint densities: log p(x_n, z = k) = log p(x_n | z = k) + log p(z = k)\n",
    "    \n",
    "    ############################################################\n",
    "\n",
    "    return log_joint_densities\n",
    "\n",
    "def compute_gmm_log_likelihood(log_joint_densities: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the log-likelihood of a GMM based on the log joint densities.\n",
    "    :param log_joint_densities: log joint densities, i.e., log p(x | z = k) p(z = k) (shape: [N, k])\n",
    "    :param weights: mixture weights, i.e., p(z) (shape: [num_components])\n",
    "    :return: log-likelihood, i.e., log p(x_n) = log sum_k p(x_n | z = k) p(z = k) (shape: [N])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO: Implement the log-likelihood log p(x_n) = log sum_k p(x_n | z = k) p(z = k) for GMMs.\n",
    "    \n",
    "    # 1.) without log_sum_exp trick (sub-optimal, don't do this!!!)\n",
    "    # marginal_densities = np.log(np.sum(np.exp(log_joint_densities), axis=1))\n",
    "\n",
    "    # 2.) with log_sum_exp trick\n",
    "    # We would like to avoid computing the bare densities np.exp(log_joint_densities)\n",
    "    #  and rather stay in log-space to avoid numerical underflow (p(x_n) might be\n",
    "    #  numerically zero and hence log p(x_n) might be undefined).\n",
    "    # Thus, subtract a_n := max_k log p(x_n, z = k) from the log p(x_n, z = k), s.t.,\n",
    "    #  log sum_k exp log p(x_n, z=k)\n",
    "    #  = log sum_k exp (log p(x_n, z=k) - a_n + a_n)\n",
    "    #  = log [ exp a_n * sum_k exp (log p(x_n, z=k) - a_n) ]\n",
    "    #  = a_n + log sum_k exp (log p(x_n, z=k) - a_n)\n",
    "    # Note that we avoid numerical underflow as one term in the sum is ensured to equal 1.\n",
    "    \n",
    "    ############################################################\n",
    "\n",
    "    return marginal_densities\n",
    "\n",
    "\n",
    "def e_step(samples: np.ndarray, weights: np.ndarray, means: np.ndarray, covariances: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    E-Step of EM for fitting GMMs. Computes responsibilities, p(z|x), given samples and model parameters.\n",
    "    :param samples: samples on which to compute the responsibilities (shape: [N, dim])\n",
    "    :param weights: weights (i.e., p(z)) of model (shape: [num_components])\n",
    "    :param means: means of components p(x|z) (shape: [num_components, dim])\n",
    "    :param covariances: covariances of model components p(x|z) (shape: [num_components, dim, dim])\n",
    "    :return: responsibilities p(z|x) (shape: [N x num_components])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO Implement the E-Step of EM for GMMs.\n",
    "    \n",
    "    log_joint_densities =   # shape (N, k)\n",
    "\n",
    "    # compute log p(z = k | x_n) = log p(x_n, z = k) - log p(x_n)\n",
    "    log_responsibilities =   # shape (N, k)\n",
    "    ############################################################\n",
    "\n",
    "    return np.exp(log_responsibilities)\n",
    "\n",
    "\n",
    "def m_step(samples: np.ndarray, responsibilities: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    M-Step of EM for fitting GMMs. Computes new parameters given samples and responsibilities p(z|x).\n",
    "    :param samples: samples (shape: [N, dim])\n",
    "    :param responsibilities: p(z|x) (shape: [N x num_components])\n",
    "    :return: - new weights p(z) (shape: [num_components])\n",
    "             - new means of components p(x|z) (shape: [num_components, dim])\n",
    "             - new covariances of components p(x|z) (shape: [num_components, dim, dim]\n",
    "    \"\"\"\n",
    "    #########################################################\n",
    "    # TODO: Implement the M-Step for EM for GMMs.\n",
    "    \n",
    "    N = samples.shape[0]\n",
    "    # k = responsibilities.shape[1]\n",
    "    # d = samples.shape[1]\n",
    "\n",
    "    # update weights: N_k := sum_n p(z=k | x_n), pi_k = N_k / N\n",
    "    \n",
    "\n",
    "    # update component means: mu_k = sum_n p(z=k | x_n) / N_k * x_n\n",
    "    sample_weights =  # shape (N, k)\n",
    "    weighted_samples =   # shape (N, k, d)\n",
    "    new_means =   # shape (k, dim)\n",
    "\n",
    "    # update component covariances: Sigma_k = sum_n p(z = k | x_n) / N_k * (x_n - mu_k)(x_n - mu_k)^T\n",
    "    diffs =   # shape (N, k, d)\n",
    "    outer_products =   # shape (N, k, d, d)\n",
    "    weighted_outer_products =   # shape (N, k, d, d)\n",
    "    new_covs =   # shape (k, d, d)\n",
    "    #########################################################\n",
    "\n",
    "    return new_weights, new_means, new_covs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c394bc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we can implement the EM-algorithm to fit a GMM to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85c634",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def fit_gaussian_mixture(samples: np.ndarray, num_components: int, num_iters: int = 30, vis_interval: int = 5):\n",
    "    \"\"\"\n",
    "    Fit a Gaussian Mixture Model using the Expectation Maximization Algorithm.\n",
    "    :param samples: samples to fit the GMM to (shape: [N, dim])\n",
    "    :param num_components: number of components of the GMM\n",
    "    :param num_iters: number of iterations\n",
    "    :param vis_interval: after how many iterations to generate the next plot\n",
    "    :return: - final weights p(z) (shape: [num_components])\n",
    "             - final means of components p(x|z) (shape: [num_components, dim])\n",
    "             - final covariances of components p(x|z) (shape: [num_components, dim, dim])\n",
    "             - log_likelihoods: log-likelihood of data under model after each iteration (shape: [num_iters])\n",
    "    \"\"\"\n",
    "    # Initialize model:\n",
    "    #  We initialize with means randomly picked from the data, unit covariances and uniform\n",
    "    #  component weights. In general, smarter initialization techniques might be necessary, e.g., k-Means.\n",
    "    initial_idx = np.random.choice(len(samples), num_components, replace=False)\n",
    "    means = samples[initial_idx]\n",
    "    covs = np.tile(np.eye(data.shape[-1])[None, ...], [num_components, 1, 1])\n",
    "    weights = np.ones(num_components) / num_components\n",
    "\n",
    "    # bookkeeping\n",
    "    log_likelihoods = np.zeros(num_iters + 1)\n",
    "\n",
    "    # iterate E- and M-steps\n",
    "    for i in range(num_iters):\n",
    "        # plotting, bookkeeping\n",
    "        if i % vis_interval == 0:\n",
    "            visualize_2d_gmm(data, weights, means, covs, title=\"Before Iteration {:02d}\".format(i))\n",
    "        log_likelihoods[i] = np.mean(compute_gmm_log_likelihood(compute_log_joint_densities(samples, weights, means, covs)))\n",
    "\n",
    "        # perform step\n",
    "        responsibilities = e_step(samples, weights, means, covs)\n",
    "        weights, means, covs = m_step(samples, responsibilities)\n",
    "\n",
    "    # final plotting, bookkeeping\n",
    "    visualize_2d_gmm(data, weights, means, covs, title=\"Final model\".format(i))\n",
    "    log_likelihoods[-1] = np.mean(compute_gmm_log_likelihood(compute_log_joint_densities(samples, weights, means, covs)))\n",
    "\n",
    "    return weights, means, covs, log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b2ad6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Finally we load some data and run the algorithm. Again, feel free to play around with the parameters a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee0429",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "seed = 42\n",
    "num_components = 3\n",
    "num_iters = 30\n",
    "vis_interval = 5\n",
    "\n",
    "# dataset\n",
    "data = np.load(\"samples_u.npy\")   # choose between samples_1.npy, samples_2.npy, samples_3.npy, samples_u.npy.\n",
    "\n",
    "# running and ploting\n",
    "np.random.seed(seed)\n",
    "final_weights, final_means, final_covariances, log_likeihoods = \\\n",
    "    fit_gaussian_mixture(data, num_components, num_iters, vis_interval)\n",
    "visualize_2d_gmm(data, final_weights, final_means, final_covariances, title=\"Final Model\")\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Log-likelihoods over iterations\")\n",
    "plt.plot(log_likeihoods)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"log-likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c35520",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. Denoising Diffusion Probabillistic Models (DDPM) (11 Points)\n",
    "In this exercise, we implement a Denoising Diffusion Probabillistic Models (DDPM). We will test the model on the same (U-shaped) data that we used to train our GMM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7f123",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa21d4e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First, we do some data pre-processing and train-test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367ae92",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(data)  # Standardize data\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val = train_test_split(X, test_size=0.2, random_state=42)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc54c9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Next, we define our denoising model $\\epsilon_{\\theta}$. Here, we use a simple MLP with a sinusoidal positional embeddings for the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d6570",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Positional Embeddings for time\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * (-np.log(10000.0) / half_dim))\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "# Define the DDPM model as simple MLP with positional embeddings\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embed_dim):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim + embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.embed = PositionalEmbedding(embed_dim)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.embed(t)\n",
    "        x = torch.cat([x, t_embed], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Model hyperparameters\n",
    "input_dim = 2\n",
    "hidden_dim = 256\n",
    "embed_dim = 32\n",
    "\n",
    "model = DDPM(input_dim, hidden_dim, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8385a0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Next, we define some parameters associated with the noise schedule. We follow the notation of the lecture and denote the noise schedule parameter as $\\alpha_t$, and define $\\beta_t = 1 -\\alpha_t$ and $ \\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d0176b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define noise schedule\n",
    "T = 50 # Number of diffusion steps\n",
    "\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "beta = torch.linspace(beta_start, beta_end, T)\n",
    "alpha = 1 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf04cc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Derivation: Forward Process (3 Points)\n",
    "Recall that the forward process involves gradually adding noise to the data over a series of steps. This is defined as:\n",
    "$q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1}, (1 - \\alpha_t) I)$\n",
    "where$ \\alpha_t$ is a noise schedule parameter.\n",
    "\n",
    "From the lecture, we know that we can express$ q(x_t \\mid x_0)$ as:\n",
    "$q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)$\n",
    "where$ \\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$.\n",
    "\n",
    "**Exercise**: Show that $q(x_{t-1} \\mid x_t, x_0)$ is:\n",
    "\n",
    "$q(x_{t-1} \\mid x_t, x_0) = \\mathcal{N}(x_{t-1}; \\bar{\\mu}_t(x_t, x_0), \\bar{\\beta}_t I)$\n",
    "\n",
    "where:\n",
    "$\\bar{\\mu}_t(x_t, x_0) = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} (1 - \\alpha_t)}{1 - \\bar{\\alpha}_t} x_0$\n",
    "\n",
    "and\n",
    "$\\bar{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} (1 - \\alpha_t)$\n",
    "\n",
    "**Hint**: It holds that $q(x_t \\mid x_{t-1}, x_0) = q(x_t \\mid x_{t-1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fffb98",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Generate Noisy Samples $x_t$ (2 Point)\n",
    "In this exercise, we want to implement the DDPM variant where we learn a denoiser $\\epsilon_{\\theta}(x_t,t)$.\n",
    "To train the model, we need to generate noisy samples from $q(x_t | x_0)$. We can obtain those by using the reparameterization trick $x_t = \\sqrt{\\bar \\alpha_t x_0} + \\sqrt{1-\\bar \\alpha_t} \\epsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a114e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def make_noisy(x_0, t):\n",
    "    \"\"\"\n",
    "    x_0: Batch of samples from the dataset. Shape: (N, D)\n",
    "    t: Batch of timesteps. Shape (N,)\n",
    "\n",
    "    Returns:\n",
    "        epsilon - random noise vector\n",
    "        x_t - noisy version of x_0\n",
    "    \"\"\"\n",
    "    ####################################\n",
    "    # # Todo: Generate noisy sample x_t\n",
    "    #\n",
    "    # return ...    \n",
    "    \n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc015a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Implementing the loss function (2 Point)\n",
    "Next, we implement the loss function for training the DDPM. The loss function is simply the MSE between the noise vector $\\epsilon$ and the predicted noise vector $\\epsilon_{\\theta}(x_t,t)$, that is $\\|\\epsilon -\\epsilon_{\\theta}(x_t,t)\\|_2$.\n",
    "\n",
    "Hint: Use the function you implemented earlier to generate a noisy sample $x_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6cfe31",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def ddpm_loss(x_0, t):\n",
    "    \"\"\"\n",
    "    x_0: Batch of samples from the dataset. Shape: (N, D)\n",
    "    t: Batch of timesteps. Shape (N,)\n",
    "\n",
    "    Returns:\n",
    "        Scaler DDPM loss\n",
    "    \"\"\"\n",
    "    loss_fn = nn.MSELoss()\n",
    "    ####################################\n",
    "    # # Todo: Implement the loss function\n",
    "    # return ...    \n",
    "    ####################################    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3edb8e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Next, we train the DDPM model using the functions that we implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43e04c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "epochs = 200 # Can also train longer for better results!\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size = 128\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for data in train_loader:\n",
    "        x_0 = data[0]\n",
    "        t = torch.randint(0, T, (x_0.size(0),), device=x_0.device).long()  # Random timestep for each sample\n",
    "        loss = ddpm_loss(x_0, t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd948c5b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Generating Samples (4 Points)\n",
    "\n",
    "Lastly, we use our trained DDPM model to generate samples. Hence, we have to implement the sample loop. To that end we iteratively denoise a sample $x_T \\sim \\mathcal{N}(0, 1)$ using\n",
    "$ x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left(\n",
    "x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar \\alpha_t}} \\epsilon_{\\theta}(x_t,t)\n",
    "\\right) + \\sqrt{1-\\alpha_t} z$ with $z \\sim \\mathcal{N}(0,1)$.\n",
    "Once fully denoised, return the sample $x_0$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239db9e5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def sample_DDPM(num_samples):\n",
    "    \"\"\"\n",
    "    num_samples: Number of samples to generate from the DDPM\n",
    "    returns: x_gen_all_t: List containing all x_t\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_gen_all_t = []\n",
    "        # Generate new samples\n",
    "        x_gen = torch.randn((num_samples, 2), device=model.fc1.weight.device)\n",
    "        x_gen_all_t.append(x_gen.cpu().numpy()) # Convert to numpy for visualization\n",
    "        for t in reversed(range(T)):\n",
    "            ####################################\n",
    "            # # Todo: Implement the denoising loop\n",
    "            #\n",
    "            # return x_gen_all_t            \n",
    "            \n",
    "            ####################################\n",
    "\n",
    "    return x_gen_all_t  # Convert to numpy for visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8485001",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Lastly, we generate samples and visualize them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b4b08",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "samples = sample_DDPM(1000)\n",
    "\n",
    "\n",
    "indices = np.linspace(0, T-1, 10, dtype=int)\n",
    "\n",
    "# Create a 2x5 grid of subplots\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "# Plot the selected samples\n",
    "for i, idx in enumerate(indices):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.scatter(samples[idx][:, 0], samples[idx][:, 1])\n",
    "    ax.set_title(f'Generated samples at t={T-idx-1}')\n",
    "    ax.set_xlim(-3, 3)  # Adjust limits if necessary\n",
    "    ax.set_ylim(-3, 3)  # Adjust limits if necessary\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747801f5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
